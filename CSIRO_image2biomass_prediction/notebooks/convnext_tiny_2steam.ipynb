{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[This helped me a lot to learn also](https://www.kaggle.com/code/none00000/csiro)    <<-- Click Me","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nimport gc\nimport os\nimport time\nimport timm\nimport torch\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nimport torch.optim as optim\nfrom sklearn.metrics import r2_score\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom albumentations import Compose, Resize, Normalize, HorizontalFlip, VerticalFlip","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:08:43.812926Z","iopub.execute_input":"2025-12-16T19:08:43.813123Z","iopub.status.idle":"2025-12-16T19:08:58.731567Z","shell.execute_reply.started":"2025-12-16T19:08:43.813106Z","shell.execute_reply":"2025-12-16T19:08:58.730936Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class CONFIG:\n    SEED = 67\n\n    TRAIN_PATH = '/kaggle/input/csiro-biomass/train.csv'\n    TEST_PATH =  '/kaggle/input/csiro-biomass/test.csv'\n    MODEL_NAME = 'convnext_tiny'\n\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    BATCH_SIZE =  4\n    NUM_WORKERS = 2\n    N_FOLDS = 5\n\n    FREEZE_EPOCHS = 1\n    LEARNING_RATE = 1e-4\n    FINETUNE_LR = 1e-5\n\n    EPOCHS = 2\n    \n    loss_weights = {\n        \"Dry_Green_g\": 0.2,\n        \"Dry_Total_g\": 0.5,\n        \"GDM_g\": 0.3\n    }\n\n    weights = {\n        \"Dry_Clover_g\": 0.1,\n        \"Dry_Dead_g\": 0.1,\n        \"Dry_Green_g\": 0.1,\n        \"Dry_Total_g\": 0.5,\n        \"GDM_g\": 0.2\n    }\n\n    IMG_SIZE = 768\n\ncfg = CONFIG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:00.812406Z","iopub.execute_input":"2025-12-16T19:09:00.813096Z","iopub.status.idle":"2025-12-16T19:09:00.818156Z","shell.execute_reply.started":"2025-12-16T19:09:00.813072Z","shell.execute_reply":"2025-12-16T19:09:00.817399Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class Transform:\n    def __init__(self):\n        self.pipeline = self.__make_pipeline()\n\n    def __make_pipeline(self):\n        base_transforms = [\n            Resize(cfg.IMG_SIZE, cfg.IMG_SIZE),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]\n            ),\n            ToTensorV2()\n        ]\n\n        original_view = Compose([\n            *base_transforms\n        ])\n\n        hflip_view = Compose([\n            HorizontalFlip(p=1.0),\n            *base_transforms\n        ])\n\n        vflip_view = Compose([\n            VerticalFlip(p=1.0),\n            *base_transforms\n        ])\n        \n        return [original_view, hflip_view, vflip_view]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:02.607695Z","iopub.execute_input":"2025-12-16T19:09:02.608295Z","iopub.status.idle":"2025-12-16T19:09:02.613772Z","shell.execute_reply.started":"2025-12-16T19:09:02.608267Z","shell.execute_reply":"2025-12-16T19:09:02.612835Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class BiomassDataset(Dataset):\n    def __init__(self, df, transform, train=True):\n        self.train = train\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.df['image_path'].iloc[idx]\n        img = np.array(Image.open(f'/kaggle/input/csiro-biomass/{path}').convert(\"RGB\"))\n\n        mid = img.shape[0] // 2\n        left, right = img[:, :mid], img[:, mid:]\n        \n        transform_left = self.transform(image=left)['image']\n        transform_right = self.transform(image=right)['image']\n        \n        if self.train:\n            targets = torch.tensor(self.df[['Dry_Green_g', 'Dry_Total_g', 'GDM_g']].iloc[idx].to_numpy(), dtype=torch.float)\n            all_targets = torch.tensor(self.df[['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']].iloc[idx].to_numpy(), dtype=torch.float)\n            return transform_left, transform_right, targets, all_targets\n        else:\n            return transform_left, transform_right","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:02.776303Z","iopub.execute_input":"2025-12-16T19:09:02.776821Z","iopub.status.idle":"2025-12-16T19:09:02.783792Z","shell.execute_reply.started":"2025-12-16T19:09:02.776798Z","shell.execute_reply":"2025-12-16T19:09:02.782903Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def weighted_r2_torch(y_true, y_pred, w):\n    w = torch.tensor(list(w.values()), dtype=torch.float32)\n    w = w / w.sum()\n    \n    y_bar = (w * y_true).sum(dim=1, keepdim=True)\n    ss_res = (w * (y_true - y_pred) ** 2).sum()\n    ss_tot = (w * (y_true - y_bar) ** 2).sum()\n    return 1 - ss_res / ss_tot\n\ndef competition_score(all_preds_3, all_targets_5):\n    \n    pred_green = all_preds_3['green']\n    pred_total = all_preds_3['total']\n    pred_gdm = all_preds_3['gdm']\n\n    pred_clover = np.maximum(0, pred_gdm - pred_green)\n    pred_dead = np.maximum(0, pred_total - pred_gdm)\n\n    y_preds = np.stack([\n        pred_clover,\n        pred_dead,\n        pred_green,\n        pred_total,\n        pred_gdm\n    ], axis=1)\n\n    y_true = all_targets_5\n\n    r2_scores = r2_score(y_true, y_preds, multioutput='raw_values')\n\n    weighted_r2_total = 0.0\n    for i, weight in enumerate(cfg.weights.values()):\n        weighted_r2_total += r2_scores[i] * weight\n\n    return weighted_r2_total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:03.033165Z","iopub.execute_input":"2025-12-16T19:09:03.033714Z","iopub.status.idle":"2025-12-16T19:09:03.039681Z","shell.execute_reply.started":"2025-12-16T19:09:03.033689Z","shell.execute_reply":"2025-12-16T19:09:03.038825Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def clean_ids(data):\n    return data.split('__')[0]\n    \ndef preprocessing(data):\n    data['sample_id'] = data['sample_id'].apply(clean_ids)\n\n    if 'target' in data.columns:\n        return data.pivot_table(\n            index=[\n                'sample_id',\n                'image_path'\n            ],\n                columns='target_name', \n                values='target'\n            ).reset_index()\n\n    data = data[['sample_id', 'image_path']]\n    return data.drop_duplicates()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:03.126866Z","iopub.execute_input":"2025-12-16T19:09:03.127692Z","iopub.status.idle":"2025-12-16T19:09:03.132576Z","shell.execute_reply.started":"2025-12-16T19:09:03.127664Z","shell.execute_reply":"2025-12-16T19:09:03.131762Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class BioModel(nn.Module):\n    def __init__(self, model_name, pretrained, n_targets=3, drop_rate=0.3):\n        super(BioModel, self).__init__()\n        self.backbone =  timm.create_model(\n            model_name,\n            pretrained=pretrained,\n            num_classes=0,\n            global_pool='avg'\n        )\n\n        self.n_features = self.backbone.num_features\n        self.n_combined_features = self.n_features * 2\n\n\n        self.head_total = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(self.n_combined_features // 2, 1) \n        )\n\n        self.head_gdm = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(self.n_combined_features // 2, 1) \n        )\n        \n        self.head_green = nn.Sequential(\n            nn.Linear(self.n_combined_features, self.n_combined_features // 2),\n            nn.ReLU(),\n            nn.Dropout(drop_rate),\n            nn.Linear(self.n_combined_features // 2, 1)\n        )\n\n    def forward(self, left, right):\n        features_left = self.backbone(left)\n        features_right = self.backbone(right)\n\n        combined = torch.cat([features_left, features_right], dim=1)\n        out_total = self.head_total(combined)\n        out_gdm = self.head_gdm(combined)\n        out_green = self.head_green(combined)\n\n        return out_total, out_gdm, out_green","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:03.288847Z","iopub.execute_input":"2025-12-16T19:09:03.289547Z","iopub.status.idle":"2025-12-16T19:09:03.296524Z","shell.execute_reply.started":"2025-12-16T19:09:03.289523Z","shell.execute_reply":"2025-12-16T19:09:03.295597Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class WeightedLoss(nn.Module):\n    def __init__(self, loss_weights_dict):\n        super(WeightedLoss, self).__init__()\n        \n        self.criterion = nn.SmoothL1Loss()\n        self.weights = loss_weights_dict\n\n    def forward(self, predictions, targets):\n        pred_green, pred_total, pred_gdm = predictions\n\n        true_green = targets[:, 0].unsqueeze(-1)\n        true_total   = targets[:, 1].unsqueeze(-1)\n        true_gdm = targets[:, 2].unsqueeze(-1)\n\n        loss_total = self.criterion(pred_total, true_total)\n        loss_gdm   = self.criterion(pred_gdm, true_gdm)\n        loss_green = self.criterion(pred_green, true_green)\n\n        total_loss = (\n            self.weights['Dry_Green_g'] * loss_green +\n            self.weights['GDM_g'] * loss_gdm +\n            self.weights['Dry_Total_g'] * loss_total\n        )\n\n        return total_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:03.581511Z","iopub.execute_input":"2025-12-16T19:09:03.582335Z","iopub.status.idle":"2025-12-16T19:09:03.588081Z","shell.execute_reply.started":"2025-12-16T19:09:03.582292Z","shell.execute_reply":"2025-12-16T19:09:03.587368Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=7, min_delta=0, verbose=False, filename='fold'):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = float('inf')\n        self.filename = filename\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score > self.best_score + self.min_delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n        torch.save(model.state_dict(), self.filename)\n        self.val_loss_min = val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:03.648744Z","iopub.execute_input":"2025-12-16T19:09:03.649480Z","iopub.status.idle":"2025-12-16T19:09:03.655592Z","shell.execute_reply.started":"2025-12-16T19:09:03.649458Z","shell.execute_reply":"2025-12-16T19:09:03.654889Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"pipeline = Transform().pipeline[0]\ntrain = pd.read_csv(cfg.TRAIN_PATH)\ntrain = preprocessing(train)\ntrain['fold'] = -1\n\n# test = pd.read_csv(cfg.TEST_PATH)\n# test = preprocessing(test)\nif len(train) > 100:\n    num_bins = 10\n\ntrain['total_bin'] = pd.cut(train['Dry_Total_g'], bins=num_bins, labels=False)\n\nskf = StratifiedKFold(\n    n_splits=cfg.N_FOLDS, \n    shuffle=True, \n    random_state=cfg.SEED\n)\n\nfor fold_num, (train_idx, valid_idx) in enumerate(skf.split(train, train['total_bin'])):\n    train.loc[valid_idx, 'fold'] = fold_num","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:03.810062Z","iopub.execute_input":"2025-12-16T19:09:03.810804Z","iopub.status.idle":"2025-12-16T19:09:03.882099Z","shell.execute_reply.started":"2025-12-16T19:09:03.810779Z","shell.execute_reply":"2025-12-16T19:09:03.881411Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_one_epoch(model, loader, criterion, optimizer, device=cfg.device):\n    model.train()  \n    epoch_loss = 0.0\n    \n    pbar = tqdm(loader, desc=\"Training\", leave=False)\n    for (img_left, img_right, train_targets, _all_targets_ignored) in pbar:\n        \n        img_left = img_left.to(device)\n        img_right = img_right.to(device)\n        targets = train_targets.to(device)\n        \n        predictions = model(img_left, img_right)\n        optimizer.zero_grad()\n        \n        loss = criterion(predictions, targets)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        pbar.set_postfix(loss=f'{loss.item():.4f}')\n        \n    return epoch_loss / len(loader)\n\ndef validate_one_epoch(model, loader, criterion, device=cfg.device):\n    model.eval()\n    epoch_loss = 0.0\n    \n    all_preds_3 = {'total': [], 'gdm': [], 'green': []}\n    all_targets_list = []\n\n    with torch.no_grad():\n        pbar = tqdm(loader, desc=\"Validating\", leave=False)\n        for (img_left, img_right, train_targets, all_targets) in pbar:\n            \n            img_left = img_left.to(device)\n            img_right = img_right.to(device)\n            train_targets = train_targets.to(device)\n            \n            pred_green, pred_total, pred_gdm = model(img_left, img_right)\n            \n            predictions_tuple = (pred_total, pred_gdm, pred_green)\n            loss = criterion(predictions_tuple, train_targets)\n            epoch_loss += loss.item()\n            \n            all_preds_3['total'].append(pred_total.cpu().numpy())\n            all_preds_3['gdm'].append(pred_gdm.cpu().numpy())\n            all_preds_3['green'].append(pred_green.cpu().numpy())\n            all_targets_list.append(all_targets.cpu().numpy())\n\n\n    preds_dict_np = {\n        'total': np.concatenate(all_preds_3['total']).flatten(),\n        'gdm':   np.concatenate(all_preds_3['gdm']).flatten(),\n        'green': np.concatenate(all_preds_3['green']).flatten()\n    }\n    targets_np_5 = np.concatenate(all_targets_list)\n    \n    score = competition_score(preds_dict_np, targets_np_5)\n    \n    avg_epoch_loss = epoch_loss / len(loader)\n    \n    return avg_epoch_loss, score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:04.162729Z","iopub.execute_input":"2025-12-16T19:09:04.163270Z","iopub.status.idle":"2025-12-16T19:09:04.172635Z","shell.execute_reply.started":"2025-12-16T19:09:04.163246Z","shell.execute_reply":"2025-12-16T19:09:04.171688Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def run_fold(fold):\n    print(f\"\\n{'='*50}\")\n    print(f\" Fold: {fold}\")\n    print(f\"{'='*50}\")\n    \n    start_time = time.time()\n    \n    train_df = train[train['fold'] != fold].reset_index(drop=True)\n    valid_df = train[train['fold'] == fold].reset_index(drop=True)\n    \n    train_dataset = BiomassDataset(train, pipeline)\n    valid_dataset = BiomassDataset(valid_df, pipeline)\n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True,\n        num_workers=cfg.NUM_WORKERS, pin_memory=True\n    )\n    \n    valid_loader = DataLoader(\n        valid_dataset, batch_size=cfg.BATCH_SIZE * 2, shuffle=False,\n        num_workers=cfg.NUM_WORKERS, pin_memory=True\n    )\n    \n    print(f\"MODEL:  '{cfg.MODEL_NAME}'...\")\n    model_base = BioModel(cfg.MODEL_NAME, cfg.MODEL_NAME)\n    \n    if torch.cuda.device_count() > 1:\n        print(f\" {torch.cuda.device_count()} GPU avilable\")\n        model = nn.DataParallel(model_base)\n    else:\n        model = model_base\n        \n    model.to(cfg.device)\n    \n    criterion = WeightedLoss(cfg.loss_weights).to(cfg.device)\n    \n    print(f\"Epochs: {cfg.EPOCHS} | FREEZE : {cfg.FREEZE_EPOCHS} | LR: {cfg.LEARNING_RATE}\")\n\n    for param in model.backbone.parameters():\n        param.requires_grad = False\n        \n    optimizer = optim.Adam(\n        filter(lambda p: p.requires_grad, model.parameters()), \n        lr=cfg.LEARNING_RATE\n    )\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.1, patience=2 \n    )\n    \n    early_stop = EarlyStopping(filename=f'{cfg.MODEL_NAME}_fold_{fold}.pt')\n\n    for epoch in range(1, cfg.FREEZE_EPOCHS + 1):\n        print(f\"\\n--- Epoch {epoch}/{cfg.EPOCHS} ---\")\n        \n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n        valid_loss, score = validate_one_epoch(model, valid_loader, criterion)\n        \n        scheduler.step(valid_loss)\n        \n        print(f\"Epoch {epoch} - Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f} | Score (R^2): {score:.4f}\")\n        \n        early_stop(score, model)\n        if early_stop.early_stop:\n            print(f'Stopping training on epoch {epoch} with best score {early_stop.best_score}')\n            break\n        \n\n    print(f\"\\n--- Fine-tuning  ---\")\n    print(f\"Epochs: {cfg.FREEZE_EPOCHS + 1}/{cfg.EPOCHS} | LR: {cfg.FINETUNE_LR}\")\n\n    for param in model.backbone.parameters():\n        param.requires_grad = True\n        \n    optimizer = optim.Adam(\n        model.parameters(), \n        lr=cfg.FINETUNE_LR\n    )\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.2, patience=3\n    )\n    \n    for epoch in range(cfg.FREEZE_EPOCHS + 1, cfg.EPOCHS + 1):\n        print(f\"\\n--- Epoch {epoch}/{cfg.EPOCHS} ---\")\n        \n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n        valid_loss, score = validate_one_epoch(model, valid_loader, criterion)\n        \n        scheduler.step(valid_loss)\n        \n        print(f\"Epoch {epoch} - Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f} | Score (R^2): {score:.4f}\")\n        \n        early_stop(score, model)\n        if early_stop.early_stop:\n            print(f'Stopping training on epoch {epoch} with best score {early_stop.best_score}')\n            break\n            \n    end_time = time.time()\n    print(f\"\\nFold {fold} runs in  {(end_time - start_time)/60:.2f}\")\n    print(f\"Best Score : {early_stop.best_score:.4f}\")\n    \n    del model, train_loader, valid_loader, train_dataset, valid_dataset\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:04.325940Z","iopub.execute_input":"2025-12-16T19:09:04.326246Z","iopub.status.idle":"2025-12-16T19:09:04.339600Z","shell.execute_reply.started":"2025-12-16T19:09:04.326225Z","shell.execute_reply":"2025-12-16T19:09:04.338834Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"try:\n    for i in range(cfg.N_FOLDS):\n        run_fold(i)\n        break\nexcept Exception as e:\n    gc.collect()\n    torch.cuda.empty_cache()\n    raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T19:09:05.810035Z","iopub.execute_input":"2025-12-16T19:09:05.810584Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n Fold: 0\n==================================================\nMODEL:  'convnext_tiny'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e437c29e8d42c088483b85c9a7259f"}},"metadata":{}},{"name":"stdout","text":"Epochs: 2 | FREEZE : 1 | LR: 0.0001\n\n--- Epoch 1/2 ---\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Train Loss: 23.1091 | Valid Loss: 20.5305 | Score (R^2): 0.0888\n\n--- Fine-tuning  ---\nEpochs: 2/2 | LR: 1e-05\n\n--- Epoch 2/2 ---\n","output_type":"stream"},{"name":"stderr","text":"Training:  21%|██        | 19/90 [00:44<02:44,  2.32s/it, loss=11.5117]","output_type":"stream"}],"execution_count":null}]}