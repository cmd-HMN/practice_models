{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59110,"databundleVersionId":6536030,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/cmdhmn/predict-health-outcomes-of-horses?scriptVersionId=253719470\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# <p style=\"font-family:Roboto Slab; font-weight:bold; letter-spacing: 2px; color:#8A2BE2; font-size:200%; text-align:left; padding: 0px; display: inline-block; border-bottom: 4px solid #4B0082;\"> Training </p>","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport gc\nimport sys\nimport pickle\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom functools import partial\n\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n\n\nfrom sklearn.metrics import f1_score\n!pip install cmaes\n!pip install optuna\nimport optuna\nfrom tqdm import tqdm\nfrom copy import deepcopy\n\nfrom sklearn.svm import SVC\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:14:15.068393Z","iopub.execute_input":"2025-07-31T19:14:15.068668Z","iopub.status.idle":"2025-07-31T19:14:31.168538Z","shell.execute_reply.started":"2025-07-31T19:14:15.068647Z","shell.execute_reply":"2025-07-31T19:14:31.167501Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <p style=\"font-family:Roboto Slab; font-weight:bold; letter-spacing: 2px; color:#8A2BE2; font-size:150%; text-align:left;padding: 0px; display: inline-block; border-bottom: 4px solid #4B0082\"> Configuration </p>","metadata":{}},{"cell_type":"code","source":"class CFG:\n    BASE_PATH = '/kaggle/input/playground-series-s3e22'\n    SAVE_PATH = '/kaggle/working'\n    TARGET = 'outcome'\n    \n    TRAIN = True\n    n_splits = 5\n    test_size = 0.3\n    random_state = 42\n    verbose = False\n\n    n_classes = 3\n\n    nan_cols = ['temp_of_extremities', 'peripheral_pulse',\n                 'capillary_refill_time','pain', 'peristalsis','abdominal_distention',\n                 'nasogastric_tube','nasogastric_reflux','rectal_exam_feces',\n                 'abdomen','abdomo_appearance']\n    \n    binary_col = [\"surgery\", \"age\", \"surgical_lesion\", \"cp_data\"]\n    \n    ohe_cols = [\"mucous_membrane\"]\n\n    # base model = \n    decision_function_shape = 'ovo'\n\n    n_estimators = 10000\n    device = 'cpu'\n\n    # optium \n    n_trials = 10000\n    early_stopping_rounds = 100\n\n    class_weights_dict = {0:0.7, 1:1, 2:0.45}\n\n    debug = False\n\n    def update_debug(self):\n        if self.debug:\n            self.n_trials = 10\n            self.n_estimators = 5\n            self.n_splits = 2\n            self.early_stopping_rounds = 5\n        \ncfg = CFG()\n\nif cfg.debug:\n    cfg.update_debug()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:14:31.170408Z","iopub.execute_input":"2025-07-31T19:14:31.171759Z","iopub.status.idle":"2025-07-31T19:14:31.179818Z","shell.execute_reply.started":"2025-07-31T19:14:31.171727Z","shell.execute_reply":"2025-07-31T19:14:31.178963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def read_data(file, cfg=cfg):\n    try:\n        return pd.read_csv(f'{cfg.BASE_PATH}/{file}.csv')\n    except:\n        print(f\"{cfg.BASE_PATH}/{file}.csv not found, Enter valid one - [train/test]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:14:31.180725Z","iopub.execute_input":"2025-07-31T19:14:31.181045Z","iopub.status.idle":"2025-07-31T19:14:31.201609Z","shell.execute_reply.started":"2025-07-31T19:14:31.181022Z","shell.execute_reply":"2025-07-31T19:14:31.200688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def score(y_true, y_pred, cfg=cfg):\n    return f1_score(y_true, y_pred, average=\"micro\")\n\ndef save_file(file_name, data, cfg=cfg):\n    with open(f\"{cfg.SAVE_PATH}/{file_name}\", 'wb') as f:\n        pickle.dump(data, f)\n\ndef load_file(file_name, cfg=cfg):\n    with open(f\"{cfg.SAVE_PATH}/{file_name}\", 'rb') as f:\n        return pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:14:31.203204Z","iopub.execute_input":"2025-07-31T19:14:31.203474Z","iopub.status.idle":"2025-07-31T19:14:31.222989Z","shell.execute_reply.started":"2025-07-31T19:14:31.203452Z","shell.execute_reply":"2025-07-31T19:14:31.222179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <p style=\"font-family:Roboto Slab; font-weight:bold; letter-spacing: 2px; color:#8A2BE2; font-size:150%; text-align:left;padding: 0px; display: inline-block; border-bottom: 4px solid #4B0082\"> Load Data </p>","metadata":{}},{"cell_type":"code","source":"train = read_data('train')\ntest = read_data('test')\nid_ = test.id\n\ntrain.drop(['id', 'lesion_3'], axis=1, inplace=True)\ntest.drop(['id', 'lesion_3'], axis=1, inplace=True)\n\ntrain.drop_duplicates(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:14:31.22385Z","iopub.execute_input":"2025-07-31T19:14:31.224137Z","iopub.status.idle":"2025-07-31T19:14:31.29853Z","shell.execute_reply.started":"2025-07-31T19:14:31.224107Z","shell.execute_reply":"2025-07-31T19:14:31.297591Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train.drop('outcome', axis=1)\ny = train['outcome'].map({'died':0, 'lived':1, 'euthanized':2})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:14:31.299445Z","iopub.execute_input":"2025-07-31T19:14:31.299755Z","iopub.status.idle":"2025-07-31T19:14:31.3061Z","shell.execute_reply.started":"2025-07-31T19:14:31.299716Z","shell.execute_reply":"2025-07-31T19:14:31.305227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <p style=\"font-family:Roboto Slab; font-weight:bold; letter-spacing: 2px; color:#8A2BE2; font-size:150%; text-align:left;padding: 0px; display: inline-block; border-bottom: 4px solid #4B0082\"> Preprocessing </p>","metadata":{}},{"cell_type":"code","source":"# taken from https://www.kaggle.com/code/yaaangzhou/pg-s3-e22-very-simple-approach\ndef preprocessing(df, le_cols, ohe_cols, cfg=cfg):\n    \n    # Label Encoding for binary cols\n    le = LabelEncoder()    \n    for col in le_cols:\n        df[col] = le.fit_transform(df[col])\n    \n    # OneHot Encoding for category cols\n    df = pd.get_dummies(df, columns=ohe_cols, dtype=int)\n    \n    df[\"pain\"] = df[\"pain\"].replace('slight', 'moderate')\n    df[\"peristalsis\"] = df[\"peristalsis\"].replace('distend_small', 'normal')\n    df[\"rectal_exam_feces\"] = df[\"rectal_exam_feces\"].replace('serosanguious', 'absent')\n    df[\"nasogastric_reflux\"] = df[\"nasogastric_reflux\"].replace('slight', 'none')\n        \n    df[\"temp_of_extremities\"] = df[\"temp_of_extremities\"].fillna(\"normal\").map({'cold': 0, 'cool': 1, 'normal': 2, 'warm': 3})\n    df[\"peripheral_pulse\"] = df[\"peripheral_pulse\"].fillna(\"normal\").map({'absent': 0, 'reduced': 1, 'normal': 2, 'increased': 3})\n    df[\"capillary_refill_time\"] = df[\"capillary_refill_time\"].fillna(\"3\").map({'less_3_sec': 0, '3': 1, 'more_3_sec': 2})\n    df[\"pain\"] = df[\"pain\"].fillna(\"depressed\").map({'alert': 0, 'depressed': 1, 'moderate': 2, 'mild_pain': 3, 'severe_pain': 4, 'extreme_pain': 5})\n    df[\"peristalsis\"] = df[\"peristalsis\"].fillna(\"hypomotile\").map({'hypermotile': 0, 'normal': 1, 'hypomotile': 2, 'absent': 3})\n    df[\"abdominal_distention\"] = df[\"abdominal_distention\"].fillna(\"none\").map({'none': 0, 'slight': 1, 'moderate': 2, 'severe': 3})\n    df[\"nasogastric_tube\"] = df[\"nasogastric_tube\"].fillna(\"none\").map({'none': 0, 'slight': 1, 'significant': 2})\n    df[\"nasogastric_reflux\"] = df[\"nasogastric_reflux\"].fillna(\"none\").map({'less_1_liter': 0, 'none': 1, 'more_1_liter': 2})\n    df[\"rectal_exam_feces\"] = df[\"rectal_exam_feces\"].fillna(\"absent\").map({'absent': 0, 'decreased': 1, 'normal': 2, 'increased': 3})\n    df[\"abdomen\"] = df[\"abdomen\"].fillna(\"distend_small\").map({'normal': 0, 'other': 1, 'firm': 2,'distend_small': 3, 'distend_large': 4})\n    df[\"abdomo_appearance\"] = df[\"abdomo_appearance\"].fillna(\"serosanguious\").map({'clear': 0, 'cloudy': 1, 'serosanguious': 2})\n    \n    # All nan_cols are object so using mode\n    for feature in cfg.nan_cols:\n        df[feature].fillna(df[feature].mode()[0], inplace=True)\n     \n    return df  \n\nX = preprocessing(X, le_cols=cfg.binary_col, ohe_cols=cfg.ohe_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:14:59.133856Z","iopub.execute_input":"2025-07-31T19:14:59.134481Z","iopub.status.idle":"2025-07-31T19:14:59.172343Z","shell.execute_reply.started":"2025-07-31T19:14:59.134451Z","shell.execute_reply":"2025-07-31T19:14:59.171304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# idea taken from https://www.kaggle.com/code/arunklenin/challenging-data-points-cosine-similarity#7.-Balance-Classes\nclass Splitter:\n    def __init__(self, cfg=cfg, kfold=True):\n        super().__init__()\n        self.test_size = cfg.test_size\n        self.kfold = kfold\n        self.n_splits = cfg.n_splits\n\n    def split_data(self, X, y):\n        if self.kfold:\n            kf = KFold(n_splits=self.n_splits, random_state=cfg.random_state, shuffle=True)\n            for train_index, val_index in kf.split(X, y):\n                X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n                y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n                yield X_train, X_val, y_train, y_val\n        else:\n            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, shuffle=True, stratify=y)\n            yield X_train, X_val, y_train, y_val\n\n\nclass Classifier:\n    def __init__(self, model_type='base', cfg=cfg):\n        super().__init__()\n        self.model_type = model_type \n        self.models = self.prepare_model()\n        self.len_models = len(self.models)\n\n\n    def prepare_model(self):\n        if self.model_type == 'base':\n            return [SVC(decision_function_shape=cfg.decision_function_shape)]\n\n        elif self.model_type == 'main':\n            xgb_params = {\n                    'n_estimators': cfg.n_estimators,\n                    'learning_rate': 0.05,\n                    'max_depth': 4,\n                    'subsample': 0.8,\n                    'colsample_bytree': 0.1,\n                    'n_jobs': -1,\n                    'eval_metric': 'merror',\n                    'objective': 'multi:softmax',\n                    'tree_method': 'hist',\n                    'verbosity': 0,\n                    'random_state': cfg.random_state,\n                    'class_weight':cfg.class_weights_dict,\n                }\n            if cfg.device == 'gpu':\n                xgb_params['tree_method'] = 'gpu_hist'\n                xgb_params['predictor'] = 'gpu_predictor'\n                    \n            xgb_params2=xgb_params.copy() \n            xgb_params2['subsample']= 0.3\n            xgb_params2['max_depth']=8\n            xgb_params2['learning_rate']=0.005\n            xgb_params2['colsample_bytree']=0.9\n    \n            \n            lgb_params = {\n                'n_estimators': cfg.n_estimators,\n                'max_depth': 8,\n                'learning_rate': 0.02,\n                'subsample': 0.20,\n                'colsample_bytree': 0.56,\n                'reg_alpha': 0.25,\n                'reg_lambda': 5e-08,\n                'objective': 'multiclass',\n                'metric': 'multi_logloss',\n                'boosting_type': 'gbdt',\n                'device': cfg.device,\n                'random_state': cfg.random_state,\n                'class_weight':cfg.class_weights_dict,\n                'verbosity': -1\n            }\n            lgb_params2 = {\n                'n_estimators': cfg.n_estimators,\n                'max_depth': 5,\n                'learning_rate': 0.05,\n                'subsample': 0.20,\n                'colsample_bytree': 0.56,\n                'reg_alpha': 0.25,\n                'reg_lambda': 5e-08,\n                'objective': 'multiclass',\n                'metric': 'multi_logloss',\n                'boosting_type': 'gbdt',\n                'device': cfg.device,\n                'random_state': cfg.random_state,\n                'class_weight':cfg.class_weights_dict,\n                'verbosity': -1\n            }\n            lgb_params3=lgb_params.copy()  \n            lgb_params3['subsample']=0.9\n            lgb_params3['reg_lambda']=0.3461495211744402\n            lgb_params3['reg_alpha']=0.3095626288582237\n            lgb_params3['max_depth']=9\n            lgb_params3['learning_rate']=0.007\n            lgb_params3['colsample_bytree']=0.5\n    \n                    \n            cb_params = {\n                'iterations': cfg.n_estimators,\n                'depth': 6,\n                'learning_rate': 0.05,\n                'l2_leaf_reg': 0.7,\n                'random_strength': 0.2,\n                'max_bin': 200,\n                'od_wait': 65,\n                'one_hot_max_size': 70,\n                'grow_policy': 'Depthwise',\n                'bootstrap_type': 'Bayesian',\n                'od_type': 'Iter',\n                'eval_metric': 'TotalF1',\n                'loss_function': 'MultiClass',\n                'task_type': cfg.device.upper(),\n                'random_state': cfg.random_state,\n            }\n            cb_sym_params = cb_params.copy()\n            cb_sym_params['grow_policy'] = 'SymmetricTree'\n            cb_loss_params = cb_params.copy()\n            cb_loss_params['grow_policy'] = 'Lossguide'\n            \n            cb_params2=  cb_params.copy()\n            cb_params2['learning_rate']=0.01\n            cb_params2['depth']=8\n            \n            cb_params3={\n                'iterations': cfg.n_estimators,\n                'random_strength': 0.1, \n                'one_hot_max_size': 70, 'max_bin': 100, \n                'learning_rate': 0.008, \n                'l2_leaf_reg': 0.3, \n                'grow_policy': 'Depthwise', \n                'depth': 9, \n                'max_bin': 200,\n                'od_wait': 65,\n                'bootstrap_type': 'Bayesian',\n                'od_type': 'Iter',\n                'eval_metric': 'TotalF1',\n                'loss_function': 'MultiClass',\n                'task_type': cfg.device.upper(),\n                'random_state': cfg.random_state,\n            }\n            models = {\n                'svc': SVC(gamma=\"auto\", probability=True, random_state=cfg.random_state),\n                'xgb': xgb.XGBClassifier(**xgb_params),\n                'xgb2': xgb.XGBClassifier(**xgb_params2),\n                'lgb': lgb.LGBMClassifier(**lgb_params),\n                'lgb2': lgb.LGBMClassifier(**lgb_params2),\n                'lgb3': lgb.LGBMClassifier(**lgb_params3),\n                'cat': CatBoostClassifier(**cb_params),\n                \"cat_sym\": CatBoostClassifier(**cb_sym_params),\n                \"cat_loss\": CatBoostClassifier(**cb_loss_params),\n                'cat2': CatBoostClassifier(**cb_params2),\n                'rf': RandomForestClassifier(n_estimators=1000, random_state=cfg.random_state),\n                'hist_gbm' : HistGradientBoostingClassifier (max_iter=300, learning_rate=0.001,  max_leaf_nodes=80,\n                                                             max_depth=6,class_weight=cfg.class_weights_dict, random_state=cfg.random_state)\n            }\n            return models\n\n        else:\n            print(f\"No such choice {self.model_type} --- valid are [main/base]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:15:01.95849Z","iopub.execute_input":"2025-07-31T19:15:01.959322Z","iopub.status.idle":"2025-07-31T19:15:01.977306Z","shell.execute_reply.started":"2025-07-31T19:15:01.959287Z","shell.execute_reply":"2025-07-31T19:15:01.976305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, cfg=cfg):\n        self.study = None\n        self.weights = None\n        self.random_state = cfg.random_state\n        self.n_trials = cfg.n_trials\n\n    def objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", -1, 2) for n in range(len(y_preds))]\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds), axis=0, weights=weights)\n        \n        weighted_pred_labels = np.argmax(weighted_pred, axis=1)\n    \n        return score(y_true, weighted_pred_labels)\n\n    def fit(self, y_true, y_preds):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n        pruner = optuna.pruners.HyperbandPruner()\n        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n        objective_partial = partial(self.objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials)\n        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n\n    def predict(self, y_preds):\n        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n        weighted_pred = np.average(np.array(y_preds), axis=0, weights=self.weights)\n        return weighted_pred\n\n    def fit_predict(self, y_true, y_preds):\n        self.fit(y_true, y_preds)\n        return self.predict(y_preds)\n    \n    def get_weights(self):\n        return self.weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:15:03.955104Z","iopub.execute_input":"2025-07-31T19:15:03.955877Z","iopub.status.idle":"2025-07-31T19:15:03.964318Z","shell.execute_reply.started":"2025-07-31T19:15:03.955849Z","shell.execute_reply":"2025-07-31T19:15:03.96344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <p style=\"font-family:Roboto Slab; font-weight:bold; letter-spacing: 2px; color:#8A2BE2; font-size:150%; text-align:left; padding: 0px; display: inline-block; border-bottom: 4px solid #4B0082;\"> Base Model </p>","metadata":{}},{"cell_type":"code","source":"c = Classifier('base')\nsvc = c.prepare_model()[0]\n\nbase_splitter = Splitter(kfold=False)\nX_train, X_val, y_train, y_val = train_test_split(X, y)\n\nsvc.fit(X_train, y_train)\n\nprint(f\"\\033[94m {score(y_val, svc.predict(X_val))}\\033[00m\")\n\n\ndel base_splitter, X_train, X_val, y_train, y_val, svc, c\ngc.collect();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:15:06.674819Z","iopub.execute_input":"2025-07-31T19:15:06.675131Z","iopub.status.idle":"2025-07-31T19:15:06.931428Z","shell.execute_reply.started":"2025-07-31T19:15:06.675107Z","shell.execute_reply":"2025-07-31T19:15:06.930607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <p style=\"font-family:Roboto Slab; font-weight:bold; letter-spacing: 2px; color:#8A2BE2; font-size:150%; text-align:left; padding: 0px; display: inline-block; border-bottom: 4px solid #4B0082;\"> Train Actual Model </p>","metadata":{}},{"cell_type":"code","source":"def train_find_weights(X, y, cfg=cfg):\n    \n    ensemble_score = []\n    ensemble_f1_score = []\n    weights = []\n    trained_models = {'svc': [],\n    'xgb': [], 'xgb2': [],\n    'lgb': [], 'lgb2': [], 'lgb3': [],\n    'cat': [], 'cat_sym': [], 'cat_loss': [], 'cat2': [],\n    'rf': [],\n    'hist_gbm': []}\n    \n    splitter = Splitter(cfg=cfg)\n    for i, (X_train, X_val, y_train, y_val) in enumerate(splitter.split_data(X, y)):\n        \n        classifier = Classifier('main', cfg)\n        models = classifier.models\n\n        oof_preds = []\n        test_preds = []\n\n        max_score = -1\n        mname = ''\n\n        print(f\"================ Fold {i + 1} ================\")\n\n        text = ''\n        pbar = tqdm(models.items(), total=len(models.items()), file=sys.stdout, colour='GREEN', unit='it')\n        for name, model in pbar:\n            if ('xgb' in name) or ('cat' in name)  :\n                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=cfg.early_stopping_rounds, verbose=cfg.verbose)\n\n            elif ('lgb' in name):\n                model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n            else:\n                model.fit(X_train, y_train)\n                \n            trained_models[f'{name}'].append(deepcopy(model))\n            \n            y_val_pred = model.predict_proba(X_val)\n    \n            y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n            f1_micro_score = score(y_val, y_val_pred_labels)\n            oof_preds.append(y_val_pred)\n            \n            text += f'{name} ----- F1 Micro Score: {f1_micro_score:.5f}\\n'\n            \n            ord_dict = {\n                \"Current\": name,\n                \"Best Model\": mname,\n                \"Best Score\": round(max_score, 2)\n            }\n\n            pbar.set_postfix(ord_dict, refresh=True)\n            if f1_micro_score > max_score:\n                max_score = f1_micro_score\n                mname = name\n                \n        optweights = OptunaWeights(cfg)\n        y_val_pred = optweights.fit_predict(y_val, oof_preds)\n        \n        y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n        f1_micro_score = score(y_val, y_val_pred_labels)\n\n        text += f'Ensemble Score ----- F1 Micro Score: {f1_micro_score:.5f}'\n        print(text)\n\n        ensemble_score.append(score)\n        ensemble_f1_score.append(f1_micro_score)\n        weights.append(optweights.weights)  \n\n    save_file('trained_models.pkl', trained_models)\n    save_file('ensemble_score.pkl', ensemble_score)\n    save_file('weights.pkl', weights)\n    save_file('ensemble_f1_score.pkl', ensemble_f1_score)\n    \n    return trained_models, weights, ensemble_score, ensemble_f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:15:25.326361Z","iopub.execute_input":"2025-07-31T19:15:25.327056Z","iopub.status.idle":"2025-07-31T19:15:25.337894Z","shell.execute_reply.started":"2025-07-31T19:15:25.327029Z","shell.execute_reply":"2025-07-31T19:15:25.336988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <p style=\"font-family:Roboto Slab; font-weight:bold; letter-spacing: 2px; color:#8A2BE2; font-size:200%; text-align:left; padding: 0px; display: inline-block; border-bottom: 4px solid #4B0082;\"> Inference </p>","metadata":{}},{"cell_type":"code","source":"def predict(test, trained_models=None, weights=None, cfg=cfg):\n    if trained_models == None:\n        trained_models = load_file('trained_models.pkl')\n\n    if weights == None:\n        weights = np.array(load_file('weights.pkl'))\n\n    test = preprocessing(test, le_cols=cfg.binary_col, ohe_cols=cfg.ohe_cols)\n\n    models = list(trained_models.keys())\n    n_folds = len(weights)\n    n_models = len(models)\n\n    # store the fold result\n    test_preds = np.zeros((test.shape[0], cfg.n_classes))\n\n    assert weights.shape == (n_folds, n_models), \"Weights shape mismatch!\"\n    \n    for i, w in enumerate(weights):\n        print(f\"================ Fold {i + 1} ================\")\n        pbar = tqdm(enumerate(models), total=n_models, file=sys.stdout, colour='GREEN', unit='model', desc=\"Predicting\")\n\n        # store the model result here\n        fold_weights = weights[i]\n        ttest = np.zeros_like(test_preds)\n        \n        for j, m in pbar:\n            model_pred = trained_models[m][i].predict_proba(test)\n            ttest += fold_weights[j] * model_pred\n\n        # only one move forward\n        test_preds += ttest / np.sum(fold_weights)\n\n    test_preds /= n_folds\n\n\n    return np.argmax(test_preds, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:15:29.356809Z","iopub.execute_input":"2025-07-31T19:15:29.357121Z","iopub.status.idle":"2025-07-31T19:15:29.364844Z","shell.execute_reply.started":"2025-07-31T19:15:29.357094Z","shell.execute_reply":"2025-07-31T19:15:29.363935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_submission(id_, preds):\n    output = pd.DataFrame()\n\n    preds = pd.Series(preds).map({0: 'died', 1:'lived', 2:'euthanized'})\n    output[\"id\"] = id_\n    output['outcome'] = preds\n\n    counts = preds.value_counts()\n\n    output.to_csv(\"submission.csv\", index=False)\n\n    plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=90, colors=['#9370DB', '#9400D3', '#A020F0'])\n    plt.title(\"Test Predictions\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:15:33.155853Z","iopub.execute_input":"2025-07-31T19:15:33.157107Z","iopub.status.idle":"2025-07-31T19:15:33.162209Z","shell.execute_reply.started":"2025-07-31T19:15:33.157078Z","shell.execute_reply":"2025-07-31T19:15:33.161404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # preproccss the train \n\n    if cfg.TRAIN:\n        print(\"========= Training =========\")\n        train_find_weights(X, y)\n\n\n    # predict test\n\n\n    print(\"\\n\"* 4)\n    print(\"========= Predicting =========\")\n    predictions = predict(test)\n\n    print(\"\\n\"* 4)\n    print(\"========= Creating Submission =========\")\n    create_submission(id_, predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T19:15:33.69562Z","iopub.execute_input":"2025-07-31T19:15:33.695916Z","iopub.status.idle":"2025-07-31T19:28:40.356079Z","shell.execute_reply.started":"2025-07-31T19:15:33.695893Z","shell.execute_reply":"2025-07-31T19:28:40.354591Z"}},"outputs":[],"execution_count":null}]}